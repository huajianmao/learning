# BIOBRIDGE: BRIDGING BIOMEDICAL FOUNDATION MODELS VIA KNOWLEDGE GRAPHS  

Zifeng Wang ∗ University of Illinois Urbana-Champaign zifengw2@illinois.edu  

Zichen Wang   
Amazon AWS AI   
zichewan@amazon.com  

Balasubramaniam Srinivasan Amazon AWS AI srbalasu@amazon.com  

Vassilis N. Ioannidis Amazon Search ivasilei@amazon.com  

Huzefa Rangwala Amazon AWS AI rhuzefa@amazon.com  

Rishita Anubhai   
Amazon AWS AI   
ranubhai@amazon.com  

# ABSTRACT  

Foundation models (FMs) learn from large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation, we present BioBRIDGE, a parameter-effcient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBRIDGE achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fne-tuning any underlying unimodal FMs. Our results demonstrate that BioBRIDGE can beat the best baseline KG embedding methods (on average by $\sim76.3\%$ ) in cross-modal retrieval tasks. We also identify BioBRIDGE demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relations. Additionally, we also show that BioBRIDGE presents itself as a general-purpose retriever that can aid biomedical multimodal question answering as well as enhance the guided generation of novel drugs. 1  

# 1 INTRODUCTION  

Foundation models (Bommasani et al., 2021) trained on large volumes of data can be leveraged and adapted for different domains. In biomedicine, FMs are trained to ingest text corpora (Gu et al., 2021) from scientifc literature, protein data in sequences and 3D-structures (Jumper et al., 2021), molecule in graphs and SMILES strings (Fabian et al., 2020) and protein-interaction data in the form of relational graphs. These pre-trained biomedical FMs have achieved a signifcant gain in comparison to previous methods trained on smaller datasets (Qiu et al., 2023). Introducing multimodal data in training further boosts the performance of FMs, especially in few-shot/zero-shot prediction settings (Radford et al., 2021). In the biomedical domain, for drug-text (Edwards et al., 2022), protein-text (Liu et al., 2023), and drug-protein data (Huang et al., 2021; Ioannidis et al., 2020), multimodal data was leveraged by the joint optimization of unimodal encoders. However, this idea encounters key issues when scaling beyond two modalities:  

Computational Cost. These approaches require many unimodal encoders with approximately similar sizes to avoid impeding each other. This setup can cause a size explosion of the bundled models in a magnitude of the number of modalities, thus rendering a computational burden for performing joint optimization.  

Data Scarcity. They require pairwise cross-modal datasets of similar size to ensure stable training. The dataset quantity increases exponentially in a combinatorial order of $\binom{K}{2}$ where $K$ represents the number of modalities, inevitably leading to data scarcity.  

![](images/3076d53de2ba8a1c9669400f69c2a08a1d396ae0ef65b0d1d00448425153f3b5.jpg)  
Figure 1: The conceptual comparison between our methods and previous methods. Left: multimodal contrastive learning, e.g., CLIP, learns from a combination of paired data, updating all unimodal encoders; Middle: ImageBind aligns all modalities with the central modality, with only the central model frozen; Right: BioBRIDGE learns the transformation across modalities from a multi-modal KG, keeping all FMs frozen.  

Unlike ImageBind (Girdhar et al., 2023), which sets the image as the central modality and aligns all the other encoders with the image via fne-tuning, the proposed BioBRIDGE keeps all the unimodal FMs fxed and learns to bridge these unimodal FMs. The conceptual demonstration is shown in Figure 1. Specifcally, BioBRIDGE learns the cross-modality transformation from biomedical knowledge graphs (KGs). This approach is modeled by leveraging the following insights:  

Data Suffciency. It is usually easier to collect unimodal data than to collect paired data from two modalities. For instance, close to 250M protein sequences (Rives et al., 2021) and 1.5B molecule structures (Sterling & Irwin, 2015) are available to perform self-supervised pre-training, while only 441K protein-text pairs are one of the largest of biological multimodal datasets (Liu et al., 2023). As such, compared to the joint training of multimodal encoders, bridging independent models that were trained on unimodal data at scale enjoys the merits of data suffciency and effciency.  

Structural Transformation. Multimodal biomedical KG contains the structure information represented by the triplets of head and tail biomedical entities and their relationships. It covers a rich set of modalities such as protein, molecule, and disease (Chandak et al., 2023), which enables comprehensive biomedical analytics and ML. We align the embedding space of unimodal FMs through a cross-modal transformation model utilizing the rich structure in KG triplets.  

In summary, BioBRIDGE aims to create a universal bridging mechanism capable of effciently connecting the representations of any pairs of modalities. Technically, the bridge modules are supervised by the rich structure information from knowledge graphs, while the unimodal FMs are kept frozen to advance the parameter and computation effciency. Experiment shows that:  

• The bridged unimodal FMs are competitive in diverse cross-modal prediction tasks.  

• BioBRIDGE can extrapolate to nodes that are not present in the training KG with comparable performance as the supervised baselines.  

• BioBRIDGE generalizes to relationships that do not exist in the training KG, and the performance can be enhanced with further training.  

# 2 RELATED WORK  

Foundation models have sparked remarkable breakthroughs in natural language processing (Brown et al., 2020) and computer vision (Kirillov et al., 2023). In biomedicine, FMs are trained with masked language modeling for text (Gu et al., 2021), proteins (Rives et al., 2021; Lin et al., 2023), drug molecules (Wang et al., 2019a), or with generative modeling for text (Taylor et al., 2022), protein (Madani et al., 2023), molecule (Bagal et al., 2021). They are trained on unimodal data and used as feature extractors in supervised prediction tasks such as protein-protein interaction (Wang et al., 2019b; Hallee & Gleghorn, 2023), protein function prediction (Gligorijevic´ et al., 2021; Wang et al., 2022a) and drug-target interaction (Sledzieski et al., 2022; Kalakoti et al., 2022).  

In the literature, multimodal biomedical FMs leverage contrastive learning on the pairs of image-text (Wang et al., 2022b), drug-protein (Huang et al., 2021), drug-text (Edwards et al., 2022) and proteintext (Liu et al., 2023; Xu et al., 2023b;a). Nonetheless, extending these approaches beyond two modalities is challenging due to the need for large volumes of pairs of multimodal datasets. A recent effort (Girdhar et al., 2023) proposed to combine pre-trained models from diverse modalities by fnetuning all unimodal encoders to be aligned with the image encoder. By contrast, BioBRIDGE does not involve the base FMs in training but trains the transformation bridge modules. In addition, the rich structure information in KG is leveraged to relax the central modality assumption and allows for a more controlled cross-modality transformation. We have also discussed the related literature in knowledge graph learning and multimodal learning in Appendix H.  

# 3 METHOD  

![](images/09ac7c921bad156d472e9ff40f306d27c2f3bdd599d4bfb7c0368077b685140c.jpg)  
Figure 2: The overall workfow of BioBRIDGE: (1) top: we train a bridge module that transforms the head node embedding to the tail node space with contrastive learning. (2) bottom left: the trained bridge module enables cross-modal prediction through the similarity search. (3) bottom right: The bridge module enables multimodal prompting for retrieval-augmented generation. The study aims to bridge embeddings across multiple modalities supervised by a KG. The overall workfow of BioBRIDGE is illustrated in Figure 2.  

Knowledge Graph (KG). KG consists of nodes $\nu$ and edges $\mathcal{E}$ as $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$ . A node in the graph, namely ${\bf v}_{i}\,=\,\{{\bf x}^{v},c^{v}\}$ , where $c^{v}$ is the node modality and $\mathbf{x}^{v}$ is the node feature. For example, a protein as a node in the KG is with $c^{v}=$ “protein” and $\mathbf{x}^{v}=$ the protein’s sequence. An edge $e\in\mathcal{E}$ that connects two nodes $\mathbf{v}_{i}$ and $\mathbf{v}_{j}$ is called a triple in the context of KG, as $\mathbf{t}_{i j}\,=\,\{\mathbf{v}_{i},\bar{\mathbf{v}}_{j},r_{i j}\}$ , where $r_{i j}$ indicates the relation between the head and tail nodes. KGs present the relations across modalities in a distributed way so BioBRIDGE does not need to set a central modality. Although traditional KG embedding (KGE) methods also enable cross-modal prediction via link prediction, they do not extrapolate to nodes not in the training KG. Instead, BioBRIDGE learns to transform the head-modality FM embeddings to the tail’s space. KG is not needed in the testing phase.  

Bridge Modalities. With foundation models (FMs) pre-trained on unimodal data, we target to bridge the unimodal FMs to fulfll multimodal tasks without fne-tuning the FMs. For two nodes from different modalities, we encode $\mathbf{v}_{i}$ and $\mathbf{v}_{j}$ into embeddings: $\mathbf{h}_{i}\ \bar{=}\ f(\mathbf{x}_{i}^{v})$ and $\mathbf{h}_{j}\;=\;g(\mathbf{x}_{j}^{v})$ where $f(\cdot)$ and $g(\cdot)$ are two FMs hence $\mathbf{h}_{i}$ and $\mathbf{h}_{j}$ are in different spaces. We aim to build the transformation that projects $\mathbf{h}_{i}$ to the space of $\mathbf{h}_{j}$ , as $\hat{\mathbf{h}}_{i}\ =\ \phi(\mathbf{h}_{i},c_{i}^{v},c_{j}^{v},r_{i j})$ that considers the modality types of two samples and their relations. As a result, the embedding $\hat{\mathbf{h}}_{i}$ is aligned with $\mathbf{h}_{j}$ and can match relevant samples from the modality $c_{j}^{v}$ by embedding-based similarity search.  

# 3.1 TRAINING & PREDICTION  

Encoding & Transformation. In the training phase, BioBRIDGE samples a triple $\mathbf{t}_{i j}$ that connects $\mathbf{v}_{i}$ and $\mathbf{v}_{j}$ from two modalities $c_{i}^{v}$ and $c_{j}^{v}$ , respectively. The bridge module $\phi$ transforms the head node embedding $\mathbf{h}_{i}$ to the space of modality $c_{j}^{v}$ to yield $\hat{\mathbf{h}}_{i}$ . Specifcally, the raw embedding $\mathbf{h}$ of a sample $\mathbf{v}\,=\,\{\mathbf{x},c\}$ is projected by the modality-specifc projection head $p$ as $\mathbf{z}=p_{c}(\mathbf{h})\in\mathbb{R}^{d}$ to ensure all embeddings follow the same dimension. We also treat $c_{i}^{v}$ , $c_{j}^{v}$ , $r_{i j}$ as categorical variables and generate their embeddings $\mathbf{c}_{i}$ , $\mathbf{c}_{j}$ , and $\mathbf{r}_{i j}$ , respectively, which all are in dimension of $d$ . The projected head node embedding $\mathbf{z}_{i}$ is transformed by  

$$
\hat{\bf h}_{i}={\bf z}_{i}+\psi({\bf z}_{i},{\bf c}_{i},{\bf c}_{j},{\bf r}_{i j}),
$$  

where $\psi:\mathbb{R}^{d}\mapsto\mathbb{R}^{d}$ generates the relation-aware transformation embeddings additive to $\mathbf{z}_{i}$ .  

Loss Function. We corrupt $\mathbf{t}_{i j}$ by replacing the tail node with others $\{\mathbf{v}_{j^{\prime}}\}_{j^{\prime}}$ where $c_{j^{\prime}.}^{v}=c_{j}^{v}$ to build informative negative samples. Based on the encoded tail nodes and the transformed head node, as $\{\hat{\mathbf{h}}_{i},\mathbf{z}_{j},\mathbf{z}_{j_{1}},\ldots\mathbf{z}_{j_{M}}\}$ , we perform contrastive learning with InfoNCE loss (Oord et al., 2018) as negative tails  

$$
\mathcal{L}_{i j}=-\log\frac{\exp(\hat{\mathbf{h}}_{i}\cdot\mathbf{z}_{j}/\tau)}{\exp(\hat{\mathbf{h}}_{i}\cdot\mathbf{z}_{j}/\tau)+\sum_{j^{\prime}\neq j}^{M}\exp(\hat{\mathbf{h}}_{i}\cdot\mathbf{z}_{j^{\prime}}/\tau)},
$$  

where $M$ is the number of the sampled negative tails; $\tau$ is a scalar temperature, and all embeddings are normalized by their $\ell_{2}$ -norm before passing to the loss function. This loss pushes $\hat{\mathbf{h}_{i}}$ close to the positive tail $\mathbf{h}_{j}$ . As the base unimodal FMs are frozen, we only update the parameters of the transformation module $\psi$ and the modality-specifc projection head $p_{c}$ .  

Prediction. Though triples extracted from KGs are used in the training, BioBRIDGE does not refer to KG for its inference. For instance, for $\mathbf{v}_{i}~=~\{\mathbf{x}_{i}^{v},c_{i}^{v}\}$ and the target modality $\mathcal{C}$ , we encode $\mathbf{v}_{j}$ , $\forall c_{j}^{v}\ \in\ c$ by the base FM $g(\cdot)$ and project them into the normalized embeddings $\mathbf{H}_{\mathcal{C}}=\{\mathbf{z}_{1},\mathbf{z}_{2},\dotsc,\mathbf{z}_{|\mathcal{C}|}\}$ . Then, we encode $\mathbf{v}_{i}$ with the base FM $f(\cdot)$ and transform it to the embedding space of $\mathcal{C}$ , yielding the normalized $\hat{\mathbf{h}}_{i}$ . We can compare the similarity of $\hat{\mathbf{h}}_{i}$ with $\mathbf{H}_{\mathcal{C}}$ effciently through matrix inner product, as $\hat{y}=\mathbf{H}_{\mathcal{C}}^{\top}\hat{\mathbf{h}}_{i}\in[-1,1]^{|\mathcal{C}|}$ .  

# 3.2 IMPLEMENTATION  

Dataset. We draw a subset of PrimeKG (Chandak et al., 2023) to build the training knowledge graph. Specifcally, we pick the six main node types from the graph: Protein, Molecule, Disease, Biological process (BP), Molecular function (MF), and Cellular component (CC) without the loss of generality. The statistics of the triples in the training KG are available in Table 8. The exact training set varies depending on the downstream evaluation datasets to avoid data leakage in our experiments. We describe how we curate the training data based on PrimeKG in Appendix B.  

Model. We categorize the six types of nodes into three modalities: protein sequence, SMILES strings, and natural language. Technically, we utilized ESM2-3B (Lin et al., 2023) to encode proteins, UniMol (Zhou et al., 2023a) to encode drug molecules, and PubMedBERT (Gu et al., 2021) to encode diseases, biological processes, molecular functions, and cellular components. For a text node, we concatenate its name and defnition to form the inputs for PubMedBERT.  

While there are many potential options to build the transformation, we deliberately choose a vanilla six-layer transformer model for the bridge module $\psi$ in Eq. 1 to verify the plausibility of the method. In detail, we stack $\mathbf{z}_{i}$ , $\mathbf{c}_{i}$ , $\mathbf{c}_{j}$ , and $\mathbf{r}_{i j}$ to build the input $\mathbf{Z}\in\mathbb{R}^{4\times d}$ for the transformers. We draw the embedding on the frst position after the transformer as the output of $\psi$ to add to the input $\mathbf{z}$ . Please refer to Appendix I for the hyperparameter setups.  

3.3 EXISTENCE OF BRIDGE MODULE AND LEARNABILITY  

Assumptions 1 (Assumptions on the neural networks and Knowledge Graph). 1. Let $K\,\in\,\mathbb{Z}^{+}$ be the total number of modalities and let $M_{1},\dots,M_{K}$ denote the $K$ neural networks trained on the different modalities whose parameters are now frozen.   
2. For every neural network $\bar{M}_{k}:\Omega_{k}\rightarrow S_{k},d>\dot{0},k\in\{1,\ldots k\},S_{k}\subseteq\mathbb{R}^{d}$ , $\Omega_{i}$ denotes the input space/ domain and the output space $S_{k}$ for every neural network is a linear subspace of $\bar{\mathbb{R}}^{d}$ . Specifcally, we will assume the dimension of every subspace is the same.   
3. Let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ denote the knowledge graph where $\mathcal{V}$ is the set of nodes and $\mathcal{E}$ is the set of edges. Each node $v\in\mathcal{V}$ belongs to one of the $K$ modalities.   
4. Every $e\,\in\,{\mathcal{E}}$ of the knowledge graph which connects two nodes $v_{i},v_{j}\in\mathcal{V}$ has an associated relation type $r_{i j}\in\mathcal{R}$ .  

Theorem 1 (Existence of a Bridge Module). For any given pair of nodes $v_{i},v_{j}\in\mathcal{V}$ of modality types $k_{v_{i}},k_{v_{j}}\;\in\;\{1,\ldots K\}$ and with representations given by their appropriate neural networks $s_{v_{i}}\in S_{k_{v_{i}}},\bar{s}_{v_{j}}\in S_{k_{v_{j}}}$ , which are connected by relation type $r_{i j}\in\mathcal{R}$ , there exists a bridge module $B:\mathbb{R}^{d}\times\{1,\dots,K\}\times\{1,\dots,K\}\times\mathcal{R}\rightarrow\mathbb{R}^{d}$ such that $B:(s_{v_{i}},k_{v_{i}},k_{v_{j}},r_{i j})\mapsto s_{v_{j}}$  

Here, we present a theorem for the existence of a unifed bridge module that connects different modalities. Please refer to Appendix A for the proof.  

# 4 EXPERIMENT: CROSS-MODALITY PREDICTION  

In this section, we perform the experiments to test the prediction capabilities of BioBRIDGE. Specifcally, the prediction tasks can be categorized into:  

• In-domain entity and relation types. Both the types of the input entity and the input relation are present in the training knowledge graph, where we conducted two series of experiments: crossmodality retrieval tasks (§4.1) and semantic similarity inference (§4.2).   
• In-domain entity and out-of-domain relation types. We consider the case where the target relations are absent in the training graph, i.e., out domain. We conducted protein-protein interaction prediction for this case (§4.3).   
• Out-of-domain entity and in-domain relation types. We also conducted experiments for outdomain entities but in-domain relations: cross-species protein-phenotype matching (§4.4).  

# 4.1 CROSS-MODALITY RETRIEVAL TASKS  

Setup. BioBRIDGE is able to perform cross-modality retrieval by matching the transformed embedding with the candidate samples in the target modality embedding space. To gauge the quality of the transformed embeddings, we compare our methods with a suite of knowledge graph embedding (KGE) methods: TransE (Bordes et al., 2013), TransD (Ji et al., 2015), TransH (Wang et al., 2014), TransR (Lin et al., 2015), ComplEx (Trouillon et al., 2016), DistMult (Yang et al., 2015), and RotatE (Sun et al., 2019), implemented with OpenKE (Han et al., 2018).  

Metric. We refer to the KG link prediction literature to use $\operatorname{Hit}(\alpha K$ $(K\,\in\,\{1,3,10\})$ ), and Mean reciprocal rank (MRR) to evaluate the prediction performance. MRR is the average reciprocal rank of all the positive test triples among the corrupted negative triples. $\operatorname{Hit}\!\bigotimes K$ measures the proportion of positive tail entities among the ranked top- $K$ possible candidates. We calculate these metrics on the direction of tail entity prediction.  

Dataset & Baseline. We split the raw PrimeKG triples to set up the cross-modality retrieval tasks. For each type of triple, we randomly sample $80\%$ , $10\%$ , and $10\%$ for the train, validation, and test sets, respectively. Then, we separate the test set by triple types, with a special focus on the predictions for: {Protein, BP/MF/CC, Interacts with}, {Drug, Disease, Indication}, {Drug, Protein, Target , Protein, Disease, Associated with , Drug, Disease, Contraindication . The statistics of the train/valid/test data used in this experiment are available in Table 10.  

Table 1: Mean reciprocal rank (MRR) on the seven cross-modal prediction tasks. “Drug $\nrightarrow$ Disease” indicates the “contraindication” relation between drug and disease. The best are in bold.   


<html><body><table><tr><td>Method</td><td>Protein→BP</td><td>Protein→MF</td><td>ProteinCC</td><td>Drug→Disease</td><td>Protein→Drug</td><td>DiseaseProtein</td><td>Drug → Disease</td></tr><tr><td>TransE</td><td>0.034</td><td>0.046</td><td>0.044</td><td>0.017</td><td>0.033</td><td>0.024</td><td>0.010</td></tr><tr><td>TransR</td><td>0.045</td><td>0.060</td><td>0.048</td><td>0.053</td><td>0.069</td><td>0.028</td><td>0.029</td></tr><tr><td>TransH</td><td>0.044</td><td>0.061</td><td>0.057</td><td>0.026</td><td>0.043</td><td>0.024</td><td>0.014</td></tr><tr><td>TransD</td><td>0.043</td><td>0.059</td><td>0.053</td><td>0.022</td><td>0.049</td><td>0.024</td><td>0.013</td></tr><tr><td>ComplEx</td><td>0.084</td><td>0.100</td><td>0.099</td><td>0.042</td><td>0.079</td><td>0.059</td><td>0.048</td></tr><tr><td>DistMult</td><td>0.054</td><td>0.089</td><td>0.095</td><td>0.025</td><td>0.044</td><td>0.033</td><td>0.047</td></tr><tr><td>RotatE</td><td>0.079</td><td>0.119</td><td>0.107</td><td>0.150</td><td>0.125</td><td>0.070</td><td>0.076</td></tr><tr><td>BioBRIDGE</td><td>0.136</td><td>0.326</td><td>0.319</td><td>0.189</td><td>0.172</td><td>0.084</td><td>0.081</td></tr></table></body></html>  

Result. We show MRR across the seven tasks in Table 1 and the breakdown performances in Tables 11, 12, 13, 14, 15, 16, and 17, respectively. We further report the overall average ranking of all methods across these tasks in Table 9. We found that BioBRIDGE is consistently ranked the best among the KGE methods. The specialized KGE algorithms learn the node and relation embeddings from scratch exclusively based on the KG, while our method builds on pre-trained FMs that already possess rich prior knowledge. As such, BioBRIDGE bridges modalities in a much more dataeffcient way. Breaking down into the performance on different tasks, as shown in Table 1, we observed that BioBRIDGE gains a higher margin over baselines on tasks with fewer triples from the KG. For instance, BioBRIDGE is around $3\times$ better than the best baseline for “Protein $\rightarrow\mathbf{M}\mathbf{F}^{\ast}$ while is around $1.6\times$ better for “Protein $\rightarrow\mathrm{BP^{\circ}}$ , which signals the beneft of BioBRIDGE in bridging FMs with limited data for multimodal tasks over training a multimodal model from scratch.  

# 4.2 SEMANTIC SIMILARITY INFERENCE  

Setup & Metric. The objective of this analysis is to evaluate the extent to which the encoded protein embeddings can capture biomolecular functional similarity, i.e., biological process (BP), molecular function (MF), and cellular component (CC). We follow the experimental protocol in (Unsal et al., 2022) that takes the gene ontology (GO) terms annotations of proteins as the target. For our method, we use the protein embeddings transformed to the BP, MF, and CC spaces as the input for evaluation. We compute the pairwise Manhattan Similarities of the encoded protein embeddings as the predictions. The fnal score is obtained by computing the Spearman’s rank correlation between the predictions and the fattened groundtruth matrix, which is the larger, the better.  

Dataset & Baseline. We leverage the test sets released by (Zhou et al., 2023b) where three $500\times500$ labeled matrices store the pairwise Lin Similarities of the protein associated BP, MF, and CC, respectively. We aggregate these matrices to obtain 1,123 unique protein sequences and remove them from the training knowledge graph to avoid data leakage. We compare our method with the following baselines: MSA Transformer (Rao et al., 2021), ESM-1B (Rives et al., 2021), ProtT5-XL (Elnaggar et al., 2021), ESM2-3B (Lin et al., 2023), OntoProtein (Zhang et al., 2022), and KeAP (Zhou et al., 2023b).  

Table 2: The comparison on semantic similarity inference across methods. The best are in bold. “Avg” is short for the average of results.   


<html><body><table><tr><td>Method</td><td>MF</td><td>BP</td><td>CC</td><td>Avg</td></tr><tr><td>MSATransformer</td><td>0.38</td><td>0.31</td><td>0.30</td><td>0.33</td></tr><tr><td>ProtT5-XL</td><td>0.57</td><td>0.21</td><td>0.40</td><td>0.39</td></tr><tr><td>ProtBERT</td><td>0.41</td><td>0.35</td><td>0.36</td><td>0.37</td></tr><tr><td>ESM-1B</td><td>0.38</td><td>0.42</td><td>0.37</td><td>0.39</td></tr><tr><td>ESM2-3B</td><td>0.33</td><td>0.42</td><td>0.23</td><td>0.32</td></tr><tr><td>OntoProtein</td><td>0.41</td><td>0.36</td><td>0.36</td><td>0.38</td></tr><tr><td>KeAP</td><td>0.41</td><td>0.41</td><td>0.40</td><td>0.41</td></tr><tr><td>BioBRIDGE</td><td>0.91</td><td>0.80</td><td>0.73</td><td>0.81</td></tr></table></body></html>  

Result. We report the results in Table 2, where our method yields a substantial improvement, with around $2\times$ better than the best baseline on average. Across the baselines, we observed the methods augmented by KG, including KeAP and OntoProtein, yield better results than the others, implying that KG connecting proteins and the biological attributes enhance protein representation learning. Nonetheless, BioBRIDGE learns to transform the protein embeddings to the biomolecular functional embedding space, thus aligning protein sequences better with the semantic meaning of functional terms. Also, the involvement of other modalities, like drugs from the KG in training, further enriches the supervision for the transformation model.  

# 4.3 PROTEIN-PROTEIN INTERACTION  

Table 3: The F1 scores of the selected methods on the protein-protein interaction task with three datasets. $\mathbf{\dot{\nabla}}^{\bullet}\mathbf{B}\mathbf{+D}^{\bullet}$ is short for the mean of BFS and DFS results. The best results are in bold.   


<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Split ID</td><td colspan="2">SHS27K</td><td rowspan="2">B+D</td><td colspan="2">SHS148K</td><td rowspan="2"></td><td colspan="2">STRING</td><td rowspan="2">B+D</td></tr><tr><td>BFS</td><td>DFS</td><td>BFS</td><td>DFS B+D</td><td>BFS</td><td>DFS</td></tr><tr><td>KeAP</td><td>0 1</td><td>0.695</td><td>0.744</td><td></td><td>0.634</td><td>0.831</td><td></td><td>0.801</td><td>0.884</td><td></td></tr><tr><td rowspan="5">ESM2-3B</td><td></td><td>0.764</td><td>0.713</td><td></td><td>0.626</td><td>0.813</td><td></td><td>0.783</td><td>0.886</td><td></td></tr><tr><td>2</td><td>0.694</td><td>0.790</td><td></td><td>0.629</td><td>0.825</td><td></td><td>0.765</td><td>0.884</td><td></td></tr><tr><td>Avg (Std)</td><td>0.718 (0.040)</td><td>0.749 (0.039)</td><td>0.733</td><td>0.630 (0.004)</td><td>0.823 (0.009)</td><td>0.726</td><td>0.783 (0.018)</td><td>0.885 (0.001)</td><td>0.834</td></tr><tr><td>0</td><td>0.687</td><td>0.733</td><td></td><td>0.637</td><td>0.839</td><td></td><td>0.801</td><td>0.880</td><td></td></tr><tr><td>1</td><td>0.768</td><td>0.725</td><td></td><td>0.644</td><td>0.827</td><td></td><td>0.781</td><td>0.884</td><td></td></tr><tr><td></td><td>2</td><td>0.690</td><td>0.791</td><td></td><td>0.632</td><td>0.819</td><td></td><td>0.778</td><td>0.881</td><td></td></tr><tr><td rowspan="4">BiOBRIDGE</td><td>Avg (Std)</td><td>0.715 (0.046)</td><td>0.750 (0.036)</td><td>0.732</td><td>0.638 (0.006)</td><td>0.828 (0.010)</td><td>0.733</td><td>0.787 (0.013)</td><td>0.882 (0.002)</td><td>0.834</td></tr><tr><td>0</td><td>0.704</td><td>0.764</td><td></td><td>0.645</td><td>0.840</td><td></td><td>0.791</td><td>0.890</td><td></td></tr><tr><td>1</td><td>0.765</td><td>0.722</td><td></td><td>0.638</td><td>0.834</td><td></td><td>0.787</td><td>0.891</td><td></td></tr><tr><td>2</td><td>0.701</td><td>0.778</td><td></td><td>0.647</td><td>0.829</td><td></td><td>0.769</td><td>0.888</td><td></td></tr><tr><td></td><td>Avg (Std)</td><td>0.723 (0.036)</td><td>0.755 (0.029)</td><td>0.739</td><td>0.643 (0.005)</td><td>0.834(0.005)</td><td>0.739</td><td>0.782 (0.012)</td><td>0.890 (0.002)</td><td>0.836</td></tr></table></body></html>  

Setup & Metric. We study the protein-protein interaction (PPI) prediction task because it represents the second experiment setup: in-domain entity and out-of-domain relation. The PPI prediction task aims to classify 7 interaction types of a pair of proteins: reaction, binding, post-translational modifcations (ptmod), activation, inhibition, catalysis, and expression. Although the ppi relation is present in PrimeKG, it only represents the physical interaction (similar to “Binding” in the seven types), while the other six types are out-of-domain.  

Following the setup of (Zhang et al., 2022), we extract the protein embeddings with the baseline pre-trained protein models, which serve as the input for a graph neural network model to be trained on the PPI network. Our method uses the protein embeddings transformed to protein space with the relation ppi. We report the F1 score for this multi-class classifcation task.  

Dataset & Baseline. Two baselines are selected for comparison: ESM2-3B (Lin et al., 2023) and KeAP (Zhou et al., 2023b). We test them on three PPI datasets: SHS27K (Chen et al., 2019), SHS148K (Chen et al., 2019), and STRING (Lv et al., 2021). Following the setup in (Zhou et al., 2023b), we perform Breadth-First Search (BFS) and Depth-First Search (DFS) to generate two train/validation/test splits, respectively.  

Result. From the results in Table 3, we observe that though the results vary across splits, our method shows a consistent improvement over the baselines in most scenarios. It is illustrated that ESM2-3B performs better than the prior state-of-the-art KeAP, which can be attributed to its pre-training on an enormous protein database. BioBRIDGE further enhances the embeddings of ESM2 by injecting the relation “ppi”, and then transforms back to the protein space. BioBRIDGE exhibits greater beneft on the datasets with fewer samples like SHS27K as it enriches the protein embedding with the protein-protein interaction ontology information. When the number of training data increases, all methods tend to converge to the same level while the baselines are still inferior to BioBRIDGE.  

# 4.4 CROSS-SPECIES PROTEIN-PHENOTYPE MATCHING  

Setup & Metric. We propose this novel task to test the capability of BioBRIDGE to handle the cross-modality transformation for out-of-domain entity and in-domain relation. As PrimeKG only contains human proteins, we build a dataset of mouse proteins and the associated mouse phenotypes from the Mouse Genome Informatics (MGI) resource (Eppig et al., 2017), acting as out-of-domain entities. We elaborate on the curation process of this data in Appendix E. Since the modality “phenotype” is absent in BioBRIDGE’s training data, we transform the encoded mouse protein embeddings to the “disease” space with the relation “associate with”. We use a suite of ranking metrics to evaluate the matching performance, including Recall $\mathbb{\@\mathbf{K}}$ , Precision $@\,\mathrm{K}$ , and $\operatorname{nDCG}@\operatorname{K}$ .  

Dataset. We build two datasets for 1) matching mouse protein (MG) to mouse phenotype (MP) and 2) matching mouse protein (MG) to human phenotype (HP). The data statistics are available in Table 18. Basically, there are 28 mouse phenotypes to predict for Task 1 and 353 human phenotypes for Task 2, respectively.  

Baseline. There is no previous work for this task. We create a dual-encoder model with a protein encoder: ESM2-1B (Lin et al., 2023) and a text encoder: PubMedBERT (Gu et al., 2021). To build the supervised baseline, we perform contrastive learning with the dual-encoder model on the paired mouse protein and phenotype.  

Table 4: The comparison between the zero-shot BioBRIDGE $^{\ast\ast}0$ -shot”) and the supervised baseline on the cross-species retrieval tasks. “Diff” indicates the relative improvement of BioBRIDGE over the supervised baseline.   


<html><body><table><tr><td colspan="4">Task1:MouseProtein toMousePhenotype</td><td colspan="4">Task 2: Mouse Protein to HumanPhenotype</td></tr><tr><td>Metric</td><td>Supervised</td><td>BioBRIDGE-O-shot</td><td>Diff</td><td>Metric</td><td>Supervised</td><td>BioBRIDGE-O-shot</td><td>Diff</td></tr><tr><td>Rec@1</td><td>0.061</td><td>0.067</td><td>10%</td><td>Prec@1</td><td>0.001</td><td>0.043</td><td>3351%</td></tr><tr><td>Prec@1</td><td>0.360</td><td>0.338</td><td>-6%</td><td>Rec@1</td><td>0.001</td><td>0.022</td><td>3098%</td></tr><tr><td>Rec@3</td><td>0.145</td><td>0.185</td><td>28%</td><td>Prec@5</td><td>0.003</td><td>0.022</td><td>624%</td></tr><tr><td>Prec@3</td><td>0.289</td><td>0.316</td><td>9%</td><td>Rec@5</td><td>0.011</td><td>0.053</td><td>393%</td></tr><tr><td>Rec@5</td><td>0.194</td><td>0.278</td><td>43%</td><td>Rec@10</td><td>0.026</td><td>0.088</td><td>236%</td></tr><tr><td>Prec@5</td><td>0.233</td><td>0.292</td><td>25%</td><td>Rec@20</td><td>0.051</td><td>0.168</td><td>228%</td></tr><tr><td>nDCG@5</td><td>0.293</td><td>0.350</td><td>20%</td><td>nDCG@20</td><td>0.019</td><td>0.082</td><td>330%</td></tr></table></body></html>  

Result. We report the results in Table 4. This is a challenging task because neither mouse proteins nor mouse/human phenotypes were used in its training data. Despite this, the underlying protein FM underwent comprehensive pre-training on protein sequences across various species. As BioBRIDGE learns to bridge human protein and human diseases, it demonstrates the emergent ability to transform mouse protein from the protein space to mouse phenotype in the text space.  

In Task 1, BioBRIDGE showcases a large margin over the supervised baseline that was fne-tuned on the paired mouse protein and phenotype data. This observation underscores the feasibility of transferring BioBRIDGE to a novel domain without further training. In Task 2, the supervised baseline failed to extrapolate to match mouse protein to human phenotype despite being learned on paired mouse proteins and mouse phenotypes. However, BioBRIDGE leverages the prior knowledge of the base FMs on protein and human disease. This inspiring result hints at the potential for novel bioinformatic analysis based on the bridged FMs enabled via cross-modal matching.  

# 5 CASE STUDY: MULTIMODAL GENERATION  

Table 5: Case study of multi-modal Q&A based on BioBRIDGE: Input a molecule/protein and a question, retrieve the relevant diseases and proteins/gene ontology terms, and prompt LLM to answer. The references are drawn for the input molecule/protein-based drug pages from DrugBank.   


<html><body><table><tr><td>Input Molecule CC[C@]1(O)C[C@ @H]2CN(C1)</td><td>Question</td><td>Retrieved Disease</td><td>Retrieved Protein TUBB8</td><td>Answer [ the main associated conditions of this small</td><td>Reference Vincristine is a vinca alkaloid</td></tr><tr><td>CCC1=C(NC3=CC=CC=C13)[C@ @](C2) [C@]4(CCN5CC=C[C@](CC)[C@@H]45) [C@ @H](OC(C)=O)[C@ ]3(O)C(=O)OC)C2=C1</td><td>What are the asso- ciated conditions and pharmaco- dynamics of this drug?</td><td>yolk sac tumor mediastinal malignant lymphoma non-small cell lung carcinoma (disease) Burkitt lymphoma malignant sex cord stromal tumor of ovary B-cell neoplasm dysgerminoma of ovary sarcoma duodenum cancer malignant Sertoli-Leydig cell tumor of ovary</td><td>TUBB TUBA4A TUBB4B TUBB2B TUBB4A TUBB3 TUBB2A TUBA1C TUBA3C</td><td>molecule drug could be cancers of the yolk sac , [..], malignant Sertoli-Leydig cell tumor of ovary The main pharmacodynamics of this small molecule drug could be its ability to bind and inhibit the activity of some of the tubulin proteins , such as TUBB8, [.] and TUBA3C. By inhibiting these proteins, the drug could disrupt the normal function of the microtubules which are filament-like structures that support the cell structure and participate in cell division. This could lead to cell death and make the cancer cells more susceptible to other treatments.</td><td>used to treat acute leukemia malignant lymphoma Hodgkin's disease acute erythraemia and acute panmyelosis . The anti- tumor activity of Vincristine is thought to be due primarily to inhibition of mitosis at metaphase through its interaction with tubulin</td></tr><tr><td>Input Protein MPTSSSTKKTQLQLEHLLLDLQMILNGI NNYKNPKLTRMLTFKFYMPKKATELKHL QCLEEELKPLEEVLNLAQSKNFHLRPRDL</td><td>Question Describe the</td><td>Retrieved Disease hepatocellular carcinoma hepatocellular adenoma</td><td>Retrieved GO Terms positive regulation J0</td><td>Answer [..] some possible associated conditions of this protein-based therapy are hepatocellular carcinoma</td><td>Ref: DrugBank: DB00541 Reference Used to treat</td></tr><tr><td>ISNINVIVLELKGSETTFMCEY ADETATIVEFLNRWITFS QSIISTLT</td><td>pharmacology of this protein-based therapy. prostate cancer</td><td>adult hepatocellular carcinoma hepatocellular clear cell carcinoma liver cancer undifferentiated carcinoma of liver and intrahepatic biliary tract nonpapillary renal cell carcinoma squamous cell carcinoma of liver and intrahepatic biliary tract familial prostate carcinoma adenocarcinoma of liver and intrahepatic biliary tract drug-induced liver injury</td><td>cell population prolifer- ation multicellular organism development signal transduction spermatogenesis cell differentiation G protein-coupled ceptor signaling path- AEM negative regulation cell population prolifer- velopment. ation</td><td>intrahepatic biliary tract The main phar- [..] and macodynamics of this therapy could be related to the regulation of cell population proliferation . cell differ- entiation, and signal transduction, which are all impor- tant processes involved in the growth and development of cancer cells. The protein sequence may also have some interactions with metal ions, proteins, and pro- teases, which could impact its ability to bind to and mod- ulate the activity of other proteins involved in cancer de-</td><td>renal cell carcinoma Aldesleukin induces the enhancement of lymphocyte mitogenesis and stimulation of long-term growth of human interleukin-2 dependent cell lines. Ref: DrugBank:DB00041</td></tr></table></body></html>  

In this section, we show how BioBRIDGE supports a multimodal generation system through crossmodal retrieval. We leverage Galactica-30B (Taylor et al., 2022) that was fne-tuned on instruction datasets as the base generator2. We prompt Galactica to answer the input question or generate target molecules following the input instruction. The used prompts are in Appendix G.  

Multiodal Question & Answer. BioBRIDGE accommodates multimodal input that consists of a molecule SMILES string/protein sequence with an associated question in natural language. In this task, BioBRIDGE serves as a cross-modal retriever to enhance the contexts for Galactica’s response to the input question. Particularly, it retrieves the following: (1) identify the potential protein targets of the input molecule, (2) identify the disease indications for the input molecule, (3) identify associated diseases for the input protein, and (4) identify gene ontology terms related to the protein. We choose the drugs from DrugBank that are not in the BioBRIDGE’s training KG. We also involve investigational drugs to test if BioBRIDGE can aid in proposing mechanisms-of-action.  

Results are shown in Table 5, with more results in Table 19 in the appendix. It demonstrates that BioBRIDGE provides key evidence that prompts Galactica to reach the right answer. For instance, BioBRIDGE can pinpoint a group of tubulin proteins and oncology-related conditions for Vincristine. This process enables Galactica to provide an accurate response, indicating that this drug inhibits the mitosis of cancer cells. In addition, in Table 19, the investigational drug Rimacalib, which has been used in trials studying the treatment of rheumatoid arthritis, is identifed by our method to possess immunomodulatory, anti-infammatory, and anti-arthritic effects. It hence prompts Galactica to reach the answer that this drug may treat diseases such as rheumatoid arthritis.  

![](images/40840a7524768ec5daeb7bfa043aad83af8f38823577600ec34ab07691bb32d5.jpg)  

Multi-Modal Generation. This task aims to achieve the text-guided generation of small-molecule drugs. We use BioBRIDGE to enrich the context for Galactica by retrieving the target proteins that are possibly associated with the target conditions. To validate the structural integrity of the generated molecules, we utilize RDKit to calculate the Tanimoto distance between the generated SMILES string and all candidate small molecule drugs listed in DrugBank. We then identify the most similar drugs. Results are shown in Table 6 with more in Table 20. We also made a baseline that prompts Galactica to generate the molecule directly, shown in Table 21. We found that (1) BioBRIDGE prompts Galactica to generate valid drug molecules; (2) the generated molecule usually shares similarities with the real drugs that were considered effective for the target condition; (3) Prompting Galactica without RAG leads to poor generation results. For instance, in Table 6, the model-generated drug is most similar to Procarbazine, which is used to treat stage III/IV Hodgkin’s disease in chemotherapy by impeding the division of cancer cells. This implies the generated drug probably fts the proposed target effect and treats lymphoma.  

# 6 CONCLUSION  

This paper investigated bridging unimodal biomedical foundation models (FM) for multimodal tasks. We identifed that BioBRIDGE can effectively transform the embeddings to the target modality, considering the types of source modality, target modality, and their relations. It is with great parameter effciency: only the bridge module needs training while all the base FMs are kept fxed, supervised by the relational information from biomedical knowledge graphs. We also identifed that BioBRIDGE can handle a diverse set of cross-modal prediction tasks by extrapolating to indomain/out-of-domain entities and relations. And the yielded performances are on par with the supervised specialist models in each task. In addition, we demonstrated how the bridged FMs can support generation tasks with multimodal inputs. In the future, we envision that BioBRIDGE can be extended to connect pre-trained FMs from other domains as long as entities across different modalities can be represented in a KG.  

# REFERENCES  

Viraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. MolGPT: molecular generation using a transformer-decoder model. Journal of Chemical Information and Modeling, 62(9):2064– 2076, 2021.  

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.  

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in Neural Information Processing Systems, 26, 2013.  

Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.  

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.  

Payal Chandak, Kexin Huang, and Marinka Zitnik. Building a knowledge graph to enable precision medicine. Scientifc Data, 10(1):67, 2023. URL https://doi.org/10.1038/ s41597-023-01960-3.  

Muhao Chen, Chelsea J-T Ju, Guangyu Zhou, Xuelu Chen, Tianran Zhang, Kai-Wei Chang, Carlo Zaniolo, and Wei Wang. Multifaceted protein–protein interaction prediction based on siamese residual rcnn. Bioinformatics, 35(14):i305–i314, 2019.  

John B Conway. A course in functional analysis, volume 96. Springer, 2019  

George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314, 1989.  

Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Conference on Empirical Methods in Natural Language Processing, 2022.  

Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):7112–7127, 2021.  

Janan T Eppig, Cynthia L Smith, Judith A Blake, Martin Ringwald, James A Kadin, Joel E Richardson, and Carol J Bult. Mouse genome informatics (mgi): resources for mining mouse genetic, genomic, and biological data in support of primary and translational research. Systems Genetics: Methods and Protocols, pp. 47–73, 2017.  

Benedek Fabian, Thomas Edlich, He´le´na Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed. Molecular representation learning with language models and domainrelevant auxiliary tasks. arXiv preprint arXiv:2011.13230, 2020.  

Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180–15190, 2023.  

Vladimir Gligorijevic´, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structurebased protein function prediction using graph convolutional networks. Nature communications, 12(1):3168, 2021.  

Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specifc language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1): 1–23, 2021.  

Logan Hallee and Jason P Gleghorn. Protein-protein interaction prediction is achievable with large language models. bioRxiv, pp. 2023–06, 2023.   
Paul R Halmos. Finite-dimensional vector spaces. Courier Dover Publications, 2017.   
Xu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi Li. OpenKE: An open toolkit for knowledge embedding. In Proceedings of EMNLP, 2018.   
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4 (2):251–257, 1991.   
Kexin Huang, Cao Xiao, Lucas M Glass, and Jimeng Sun. MolTrans: molecular interaction transformer for drug–target interaction prediction. Bioinformatics, 37(6):830–836, 2021.   
Vassilis N. Ioannidis, Xiang Song, Saurav Manchanda, Mufei Li, Xiaoqin Pan, Da Zheng, Xia Ning, Xiangxiang Zeng, and George Karypis. Drkg - drug repurposing knowledge graph for covid-19. https://github.com/gnn4dr/DRKG/, 2020.   
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding via dynamic mapping matrix. In Proceedings of ACL, pp. 687–696, 2015.   
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zˇ ´ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.   
Yogesh Kalakoti, Shashank Yadav, and Durai Sundar. Transdti: transformer-based language models for estimating dtis and building a drug recommendation workfow. ACS omega, 7(3):2706–2717, 2022.   
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 29, 2015.   
Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123–1130, 2023.   
Shengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Anthony Gitter, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar. A text-guided protein design framework. arXiv preprint arXiv:2302.04611, 2023.   
Shuwen Liu, Bernardo Grau, Ian Horrocks, and Egor Kostylev. Indigo: Gnn-based inductive knowledge graph completion using pair-wise encoding. Advances in Neural Information Processing Systems, 34:2034–2045, 2021.   
Xinyu Lu, Lifang Wang, Zejun Jiang, Shichang He, and Shizhong Liu. MMKRL: A robust embed  
ding approach for multi-modal knowledge graph representation learning. Applied Intelligence, pp. 1–18, 2022.   
Guofeng Lv, Zhiqiang Hu, Yanguang Bi, and Shaoting Zhang. Learning unknown from correlations: graph neural network for inter-novel-protein interaction prediction. arXiv preprint arXiv:2105.06709, 2021.   
Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos Jr, Caiming Xiong, Zachary Z Sun, Richard Socher, et al. Large language models generate functional protein sequences across diverse families. Nature Biotechnology, pp. 1–8, 2023.   
Hatem Mousselly-Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth. A multimodal translation-based approach for knowledge graph representation learning. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pp. 225–234, 2018.  

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. In NeurIPS, 2018.  

Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang Zhang, Yinzhao Dong, Kyle Lam, Frank P-W Lo, Bo Xiao, et al. Large ai models in health informatics: Applications, challenges, and the future. arXiv preprint arXiv:2303.11568, 2023.  

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021.  

Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pp. 8844–8856. PMLR, 2021.  

Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert RG Lanckriet, Roger Levy, and Nuno Vasconcelos. A new approach to cross-modal multimedia retrieval. In Proceedings of the 18th ACM international conference on Multimedia, pp. 251–260, 2010.  

Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.  

Samuel Sledzieski, Rohit Singh, Lenore Cowen, and Bonnie Berger. Adapting protein language models for rapid dti prediction. bioRxiv, pp. 2022–11, 2022.  

Teague Sterling and John J Irwin. Zinc 15–ligand discovery for everyone. Journal of chemical information and modeling, 55(11):2324–2337, 2015.  

Gilbert Strang. Linear algebra and its applications. 2012.  

Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197, 2019.  

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.  

Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. In International Conference on Machine Learning, pp. 9448–9457. PMLR, 2020.  

The´o Trouillon, Johannes Welbl, Sebastian Riedel, E´ ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International Conference on Machine Learning, pp. 2071–2080. PMLR, 2016.  

Serbulent Unsal, Heval Atas, Muammer Albayrak, Kemal Turhan, Aybar C Acar, and Tunca Dog˘an. Learning functional properties of proteins with language models. Nature Machine Intelligence, 4 (3):227–245, 2022.  

Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. SMILES-BERT: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, pp. 429–436, 2019a.  

Yanbin Wang, Zhu-Hong You, Shan Yang, Xiao Li, Tong-Hai Jiang, and Xi Zhou. A high effcient biological language model for predicting protein–protein interactions. Cells, 8(2):122, 2019b.  

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 28, 2014.  

Zichen Wang, Steven A Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O Salawu, Colby J Wise, Sri Priya Ponnapalli, et al. Lm-gvp: an extensible sequence and structure informed deep learning framework for protein property prediction. Scientifc reports, 12(1):6832, 2022a.   
Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical images and text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3876–3887, 2022b.   
Ruobing Xie, Stefan Heinrich, Zhiyuan Liu, Cornelius Weber, Yuan Yao, Stefan Wermter, and Maosong Sun. Integrating image-based and knowledge-based representation learning. IEEE Transactions on Cognitive and Developmental Systems, 12(2):169–178, 2019.   
Hanwen Xu, Addie Woicik, Hoifung Poon, Russ B Altman, and Sheng Wang. Multilingual translation for zero-shot biomedical classifcation using biotranslator. Nature Communications, 14(1): 738, 2023a.   
Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-modality learning of protein sequences and biomedical texts. In ICML, 2023b.   
Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of the International Conference on Learning Representations (ICLR) 2015, 2015.   
Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Qiang Zhang, Jiazhang Lian, and Huajun Chen. Ontoprotein: Protein pretraining with gene ontology embedding. In International Conference on Learning Representations, 2022.   
Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-Mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2023a.   
Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Bian Cheng, and Yizhou Yu. Protein representation learning via knowledge enhanced primary structure reasoning. In The Eleventh International Conference on Learning Representations, 2023b.  

# A PROVING EXISTENCE OF BRIDGE MODULE AND LEARNABILITY  

This section aims to show the existence of a bridge module, followed by its learnability. Given our setup of unimodal FMs, we will show the existence of transformation modules that can take us from any modality’s output space to another under certain assumptions. We will proceed to frst state our assumptions and then the statement, followed by the proof for the statement.  

Assumptions 1 (Assumptions on the neural networks and Knowledge Graph). 1. Let $K\,\in\,\mathbb{Z}^{+}$ be the total number of modalities and let $M_{1},\dots,M_{K}$ denote the $K$ neural networks trained on the different modalities whose parameters are now frozen.   
2. For every neural network $\hat{M}_{k}:\Omega_{k}\rightarrow S_{k},d>\stackrel{\cdot}{0},k\in\{1,\ldots k\},S_{k}\subseteq\mathbb{R}^{d}$ , $\Omega_{i}$ denotes the input space/ domain and the output space $S_{k}$ for every neural network is a linear subspace of $\bar{\mathbb{R}}^{d}$ . Specifcally, we will assume the dimension of every subspace is the same.   
3. Let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ denote the knowledge graph where $\mathcal{V}$ is the set of nodes and $\mathcal{E}$ is the set of edges. Each node $v\in\mathcal{V}$ belongs to one of the $K$ modalities.   
4. Every $e\,\in\,{\mathcal{E}}$ of the knowledge graph which connects two nodes $v_{i},v_{j}\in\mathcal{V}$ has an associated relation type $r_{i j}\in\mathcal{R}$ .  

Theorem 1 (Existence of a Bridge Module). For any given pair of nodes $v_{i},v_{j}\in\mathcal{V}$ of modality types $k_{v_{i}},k_{v_{j}}\;\in\;\{1,\ldots K\}$ and with representations given by their appropriate neural networks $s_{v_{i}}\in S_{k_{v_{i}}},\bar{s}_{v_{j}}\in S_{k_{v_{j}}}$ , which are connected by relation type $r_{i j}\in\mathcal{R}$ , there exists a bridge module $B:\mathbb{R}^{d}\times\{1,\dots,K\}\times\{1,\dots,K\}\times\mathcal{R}\rightarrow\mathbb{R}^{d}$ such that $B:(s_{v_{i}},k_{v_{i}},k_{v_{j}},r_{i j})\mapsto s_{v_{j}}$  

Proof. At a very high level, a direct consequence of the Hahn-Banach Theorem (Conway, 2019) can be used to see that given a Euclidean space and given two compact, convex subsets of that Euclidean space, there exists a continuous function mapping between them (given that there is a linear functional that separates them). Given our assumption that our subspaces are linear and therefore convex, as well as the assumption of continuity of neural networks, we can quickly assert the existence of transformations between the spaces.  

Here, however, we will go into the details and show a simple proof using linear algebra. We will use a multi-step approach to prove the existence of the bridge module. Firstly, we will look at expressibility in terms of linear subspaces and then introduce conditional transformations based on relation types. Subsequently, we will look at the compositionality and aggregation of paths (as we are dealing with knowledge graphs) for the bridge module. We will specifcally look at linear subspaces here, but similar arguments can also be established for convex hull-like spaces or other spaces that have an established geometry.  

First, we look at the linear mapping between linear subspaces. Given any two linear subspaces $S_{x},S_{y}$ , a linear transformation between the two is one which maps points in $S_{x}$ to $S_{y}$ i.e. $T_{x y}:$ $S_{x}\to S_{y}$ - and this can always be represented using a matrix $A_{x y}$ as $T_{x y}(p)\,=\,A_{x y}\,\dot{\cdot}\,p,p\,\in\,\dot{S}_{x}$ . The existence of such a transformation is guaranteed by the rank nullity theorem (Strang, 2012) without any loss of information (as the dimensions of the subspaces are taken to be the same by construction).  

Next, any relation in $r_{i j}\,\in\,\mathcal R$ can be represented as a vector, i.e., a relation embedding $r(r_{i j})\in$ $\mathbb{R}^{p},p\,>\,0$ and the transformation $T$ between subspaces can be a function over the matrix $A_{x y}$ as well as $r(r_{i j})$ i.e. $A_{x y}^{a}\,=\,f(A_{x y},r(r_{i j}))$ and the conditional transformation (relation dependent) of any point $p\in S_{x}$ via relation $r_{i j}$ is now given as $T_{x y}^{r i j}\left(p\right)\,=\,A_{x y}^{r i j}\,\cdot\,p$ . Such a formulation also allows us to directly model compositionality of transformations (due to the properties of matrices) as given two transformations $T_{x y},T_{y z}$ between $S_{x}\rightarrow S_{y}\rightarrow S_{z}$ , a composition $T_{y z}\circ T_{x y}$ (given by $A_{y z}^{r_{j k}}\cdot A_{x y}^{r_{i j}}$ in matrix notation) captures a transformation from $S_{x}$ to $S_{z}$ . Given that we are dealing with linear subspaces and transformations, these compositions are always defned (Strang, 2012).  

Next, we look at the aggregation of multiple paths - since we are dealing with KGs, there can be multiple paths between any two nodes. In such cases, the transformation between any two subspaces can then be obtained as a weighted combination of the paths, i.e., $A_{a g g}\:=\:\Sigma_{i=1}^{N}w_{i}A_{i}$ such that $\Sigma_{i=1}^{N}w_{i}=1$ where $w_{i}$ is the weight associated with a path and $A$ gives the transformation matrix between the spaces. When all $w_{i}>0$ , this forms a convex combination and ensures that the aggregated transformation is still within the space by the original transformations (Boyd & Vandenberghe, 2004) and therefore confrms the existence of a unifed bridge module. 口  

Now, given the existence of the above transformations as defned by the KG, the convergence of sequences of linear transformations (Halmos, 2017), the universality of neural networks (Hornik, 1991; Cybenko, 1989) and the continuity of the unimodal FM’s ensure that such conditional transformations (given by relation types) can be approximated when appropriate loss functions are employed via SGD.  

# B ESTABLISHING TRAINING DATA BASED ON PRIMEKG  

Table 7: The statistics of nodes and edges of the raw PrimeKG data and the processed KG for training BioBRIDGE. “Original”: the original PrimeKG; “Processed”: the processed KG; “Dropped”: how many samples are dropped in the processing.   


<html><body><table><tr><td>Nodes Modality</td><td></td><td></td><td>Dropped</td><td>Percent dropped</td></tr><tr><td>biological process</td><td>Original 28,642</td><td>Processed 27,478</td><td>1,164</td><td>4.06%</td></tr><tr><td>protein</td><td>27,671</td><td>19,162</td><td>8,509</td><td>30.75%</td></tr><tr><td>disease</td><td>17,080</td><td>17,080</td><td>0</td><td>0.00%</td></tr><tr><td>molecular function</td><td>11,169</td><td>10,966</td><td>203</td><td>1.82%</td></tr><tr><td>drug</td><td>7,957</td><td>6,948</td><td>1,009</td><td>12.68%</td></tr><tr><td>cellular component</td><td>4,176</td><td>4,013</td><td>163</td><td>3.90%</td></tr><tr><td>Summation</td><td>96,695</td><td>85,647</td><td>11,048</td><td>11.43%</td></tr><tr><td>Edges</td><td></td><td></td><td></td><td></td></tr><tr><td>Relation type</td><td>Original</td><td>Processed</td><td>Dropped</td><td>Percent Dropped</td></tr><tr><td>drug-drug</td><td>2,672,628</td><td>2,241,466</td><td>431,162</td><td>16.13%</td></tr><tr><td>protein_protein</td><td>642,150</td><td>629,208</td><td>12,942</td><td>2.02%</td></tr><tr><td>bioprocess_protein</td><td>289,610</td><td>272,642</td><td>16,968</td><td>5.86%</td></tr><tr><td>cellcomp-protein</td><td>166,804</td><td>149,504</td><td>17,300</td><td>10.37%</td></tr><tr><td>disease_protein</td><td>160,822</td><td>155,924</td><td>4,898</td><td>3.05%</td></tr><tr><td>molfunc-protein</td><td>139,060</td><td>133,522</td><td>5,538</td><td>3.98%</td></tr><tr><td>bioprocess_bioprocess</td><td>105,772</td><td>99,630</td><td>6,142</td><td>5.81%</td></tr><tr><td>disease_disease</td><td>64,388</td><td>64,388</td><td>0</td><td>0.00%</td></tr><tr><td>contraindication</td><td>61,350</td><td>60,130</td><td>1,220</td><td></td></tr><tr><td>drug-protein</td><td>51,306</td><td>47,614</td><td></td><td>1.99%</td></tr><tr><td>molfunc_molfunc</td><td>27,148</td><td></td><td>3,692</td><td>7.20%</td></tr><tr><td></td><td></td><td>26,436</td><td>712</td><td>2.62%</td></tr><tr><td>indication</td><td>18,776</td><td>17,578</td><td>1,198</td><td>6.38%</td></tr><tr><td>cellcomp_cellcomp</td><td>9,690</td><td>9,200</td><td>490</td><td>5.06%</td></tr><tr><td>off-label use Summation</td><td>5,136 4,414,640</td><td>4,998 3,912,240</td><td>138 502,400</td><td>2.69% 11.38%</td></tr></table></body></html>  

This paper aims to make a proof of concept in bridging uni-modal FMs, so it simplifes the KG in the experiment by only keeping six modalities. Nonetheless, protein, disease, drug, and gene ontology terms are the main biomedical entities covering the majority of real-world biomedical tasks (e.g., drug discovery, repurposing, PPI, protein function prediction, drug target identifcation, drug-target interaction prediction).  

The raw PrimeKG provides a bind of biomedical entities from a widespread of sources. However, it does not provide the associated properties for all the entities, e.g., the sequence structure of the proteins. To train BioBRIDGE, we try to link the entities to the external knowledge bases and flter out those without the required property. The statistics of raw KG and the processed are available in Table 7. We describe the data curation process for each type of entities as the following.  

Protein Data. There are 27,671 proteins in total in the original PrimeKG. We try to match the provided NCBI gene ID of the proteins to the UniProtKB/Swiss-Prot sequence database, via the uniprot id-mapping tool https://www.uniprot.org/id-mapping. We were able to retrieve 27,478 protein sequences that are matched with the gene ID.  

We delve deeper into the unmapped proteins and fnd most are non-protein-coding genes (pseudogenes, rRNA, ncRNA genes), which are genes unable to produce proteins. It is hence reasonable to ignore them for protein-centric tasks.  

Drug Data. There are 7,957 drugs in the original PrimeKG. We try to match the offered DrugBank ID to the database https://go.drugbank.com/drugs. We dropped drugs without SMILES strings to obtain a total of 6,948 drugs.  

Gene Ontology Terms. Biological process, molecular function, and cellular component are represented by all gene ontology (GO) terms. We leveraged AmiGO for searching the detailed descriptions of the GO terms through their IDs https://amigo.geneontology.org/amigo/ search/ontology. We were able to retrieve the features of the most GO terms then kept 27,478 BP, 10,966 MF, and 4,013 MF, in the training data.  

Disease. The descriptions of those diseases are provided by the raw PrimeKG data so we are able to keep all 17,080 diseases for training.  

# C DATA STATISTICS OF THE KNOWLEDGE GRAPH  

Table 8: The data statistics of the training knowledge graph drawn from PrimeKG.   


<html><body><table><tr><td>Head Node</td><td>Tail Node</td><td>Relation</td><td>Number</td></tr><tr><td rowspan="9">protein</td><td>protein</td><td>ppi</td><td>642,150</td></tr><tr><td>biological process</td><td>interacts with</td><td>144,805</td></tr><tr><td>cellular component</td><td>interacts with</td><td>83,402</td></tr><tr><td>molecular function</td><td>interacts with</td><td>69,530</td></tr><tr><td>disease</td><td>associated with</td><td>80,411</td></tr><tr><td>snp</td><td>target</td><td>16,380</td></tr><tr><td></td><td>enzyme</td><td>5,317</td></tr><tr><td></td><td>transporter</td><td>3,092</td></tr><tr><td></td><td>carrier</td><td>864</td></tr><tr><td rowspan="9">drug</td><td>drug</td><td>synergistic interaction</td><td>2,672,628</td></tr><tr><td>disease</td><td>contraindication</td><td>30,675</td></tr><tr><td></td><td>indication</td><td>9,388</td></tr><tr><td></td><td>off-label use</td><td>2,568</td></tr><tr><td>protein</td><td>target</td><td>16,380</td></tr><tr><td></td><td>enzyme</td><td>5,317</td></tr><tr><td></td><td>transporter</td><td>3,092</td></tr><tr><td></td><td>carrier</td><td>864</td></tr><tr><td>disease</td><td>parent-child</td><td>64,388</td></tr><tr><td rowspan="4">disease</td><td>gene/protein</td><td></td><td></td></tr><tr><td></td><td>associated with</td><td>80,411</td></tr><tr><td>Bn.ip</td><td>contraindication</td><td>30,675</td></tr><tr><td></td><td>indication off-label use</td><td>9,388 2,568</td></tr></table></body></html>  

# D CROSS-MODALITY RETRIEVAL TASKS  

Table 9: The mean and standard deviation (in parenthesis) of the ranks of all compared methods across the seven cross-modality retrieval tasks. Lower rank is better. Best are in bold.   


<html><body><table><tr><td>Method</td><td>Rank-MRR （↑）</td><td>Rank-Hits@10 (↓)</td><td>Rank-Hits@3(↓)</td><td>Rank-Hits@1 (↓)</td></tr><tr><td>TransE</td><td>7.71 (0.00)</td><td>7.71 (0.00)</td><td>7.43 (0.52)</td><td>5.00 (0.00)</td></tr><tr><td>TransR</td><td>5.00 (1.41)</td><td>4.57 (1.63)</td><td>5.14 (1.47)</td><td>5.00 (0.00)</td></tr><tr><td>TransH</td><td>5.86 (0.82)</td><td>5.71 (0.55)</td><td>6.00 (0.41)</td><td>5.00 (0.00)</td></tr><tr><td>TransD</td><td>6.71 (0.84)</td><td>6.57 (0.82)</td><td>6.57 (0.82)</td><td>5.00 (0.00)</td></tr><tr><td>ComplEx</td><td>3.00 (0.63)</td><td>3.43 (0.55)</td><td>3.00 (0.63)</td><td>2.86 (0.63)</td></tr><tr><td>DistMult</td><td>4.57 (1.03)</td><td>5.00 (1.67)</td><td>4.00 (1.79)</td><td>3.14 (0.89)</td></tr><tr><td>RotatE</td><td>2.14 (0.41)</td><td>2.00 (0.00)</td><td>2.43 (0.84)</td><td>2.86 (0.98)</td></tr><tr><td>BioBRIDGE</td><td>1.00 (0.00)</td><td>1.00 (0.00)</td><td>1.00 (0.00)</td><td>1.00 (0.00)</td></tr></table></body></html>  

Table 10: The data statistics of the train/valid/test splits used in cross-modality retrieval tasks.   


<html><body><table><tr><td>Triple Type</td><td>#HeadNode</td><td>#TailNode</td><td>#Train Triple</td><td>#Valid Triple</td><td>#TestTriple</td></tr><tr><td>Protein-Interactswith-BP</td><td>16,808</td><td>12,047</td><td>111,649</td><td>11,620</td><td>13,052</td></tr><tr><td>Protein-Interactswith-MF</td><td>17,289</td><td>4,284</td><td>55,252</td><td>5,438</td><td>6,071</td></tr><tr><td>Protein-Interactswith-CC</td><td>17,211</td><td>1,684</td><td>61,471</td><td>6,250</td><td>7,031</td></tr><tr><td>Protein-Associatedwith-Disease</td><td>8,548</td><td>5,566</td><td>63,869</td><td>6,659</td><td>7,434</td></tr><tr><td>Drug-Indication-Disease</td><td>1,592</td><td>1,279</td><td>7,290</td><td>704</td><td>795</td></tr><tr><td>Drug-Contraindiation-Disease</td><td>1,217</td><td>1,181</td><td>24,423</td><td>2,676</td><td>2,966</td></tr><tr><td>Drug - Target - Protein</td><td>5,074</td><td>2,620</td><td>12,952</td><td>920</td><td>1,055</td></tr></table></body></html>  

Table 11: Cross-modality prediction performance for triple type: “Protein - Interacts with - BP”.   


<html><body><table><tr><td>Method</td><td>MRR</td><td>Hit@10</td><td>Hit@3</td><td>Hit@1</td><td>MRRRank</td><td>Hit@10Rank</td><td>Hit@3Rank</td><td>Hit@1Rank</td></tr><tr><td>TransE</td><td>0.034</td><td>0.097</td><td>0.020</td><td>0.000</td><td>8</td><td>8</td><td>8</td><td>5</td></tr><tr><td>TransR</td><td>0.045</td><td>0.136</td><td>0.023</td><td>0.000</td><td>5</td><td>4</td><td>5</td><td>5</td></tr><tr><td>TransH</td><td>0.044</td><td>0.129</td><td>0.023</td><td>0.000</td><td>6</td><td>6</td><td>6</td><td>5</td></tr><tr><td>TransD</td><td>0.043</td><td>0.125</td><td>0.020</td><td>0.000</td><td>7</td><td>7</td><td>7</td><td>5</td></tr><tr><td>ComplEx</td><td>0.084</td><td>0.218</td><td>0.067</td><td>0.020</td><td>2</td><td>3</td><td>2</td><td>2</td></tr><tr><td>DistMult</td><td>0.054</td><td>0.132</td><td>0.045</td><td>0.013</td><td>4</td><td>5</td><td>4</td><td>3</td></tr><tr><td>RotatE</td><td>0.079</td><td>0.239</td><td>0.057</td><td>0.012</td><td>3</td><td>2</td><td>3</td><td>4</td></tr><tr><td>BioBRIDGE</td><td>0.136</td><td>0.282</td><td>0.146</td><td>0.066</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table></body></html>  

Table 12: Cross-modality prediction performance for triple type: “Protein - Interacts with - MF”.   


<html><body><table><tr><td>Method</td><td>MRR</td><td>Hit@10</td><td>Hit@3</td><td>Hit@1</td><td>MRRRank</td><td>Hit@10Rank</td><td>Hit@3Rank</td><td>Hit@1Rank</td></tr><tr><td>TransE</td><td>0.046</td><td>0.143</td><td>0.030</td><td>0.000</td><td>8</td><td>8</td><td>8</td><td>5</td></tr><tr><td>TransR</td><td>0.060</td><td>0.196</td><td>0.041</td><td>0.000</td><td>6</td><td>6</td><td>5</td><td>5</td></tr><tr><td>TransH</td><td>0.061</td><td>0.199</td><td>0.040</td><td>0.000</td><td>5</td><td>5</td><td>6</td><td>5</td></tr><tr><td>TransD</td><td>0.059</td><td>0.185</td><td>0.039</td><td>0.000</td><td>7</td><td>7</td><td>7</td><td>5</td></tr><tr><td>ComplEx</td><td>0.100</td><td>0.289</td><td>0.080</td><td>0.020</td><td>3</td><td>3</td><td>3</td><td>4</td></tr><tr><td>DistMult</td><td>0.089</td><td>0.203</td><td>0.080</td><td>0.033</td><td>4</td><td>4</td><td>3</td><td>2</td></tr><tr><td>RotatE</td><td>0.119</td><td>0.360</td><td>0.103</td><td>0.021</td><td>2</td><td>2</td><td>2</td><td>3</td></tr><tr><td>BioBRIDGE</td><td>0.326</td><td>0.653</td><td>0.382</td><td>0.177</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table></body></html>  

Table 13: Cross-modality prediction performance for triple type: “Protein - interacts with - CC”.   


<html><body><table><tr><td>Method</td><td>MRR</td><td>Hit@10</td><td>Hit@3</td><td>Hit@1</td><td>MRRRank</td><td>Hit@10Rank</td><td>Hit@3Rank</td><td>Hit@1Rank</td></tr><tr><td>TransE</td><td>0.044</td><td>0.133</td><td>0.026</td><td>0.000</td><td>8</td><td>8</td><td>7</td><td>5</td></tr><tr><td>TransR</td><td>0.048</td><td>0.142</td><td>0.023</td><td>0.000</td><td>7</td><td>7</td><td>8</td><td>5</td></tr><tr><td>TransH</td><td>0.057</td><td>0.176</td><td>0.029</td><td>0.000</td><td>5</td><td>5</td><td>5</td><td>5</td></tr><tr><td>TransD</td><td>0.053</td><td>0.162</td><td>0.026</td><td>0.000</td><td>6</td><td>6</td><td>6</td><td>5</td></tr><tr><td>ComplEx</td><td>0.099</td><td>0.267</td><td>0.081</td><td>0.023</td><td>3</td><td>3</td><td>3</td><td>3</td></tr><tr><td>DistMult</td><td>0.095</td><td>0.212</td><td>0.092</td><td>0.034</td><td>4</td><td>4</td><td>2</td><td>2</td></tr><tr><td>RotatE</td><td>0.107</td><td>0.330</td><td>0.079</td><td>0.018</td><td>2</td><td>2</td><td>4</td><td>4</td></tr><tr><td>BioBRIDGE</td><td>0.319</td><td>0.676</td><td>0.373</td><td>0.160</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table></body></html>  

oss-modality prediction performance for triple type: “Drug - Indication -   


<html><body><table><tr><td>Method</td><td>MRR</td><td>Hit@10</td><td>Hit@3</td><td>Hit@1</td><td>MRRRank</td><td>Hit@10Rank</td><td>Hit@3Rank</td><td>Hit@1Rank</td></tr><tr><td>TransE</td><td>0.017</td><td>0.057</td><td>0.006</td><td>0.000</td><td>8</td><td>8</td><td>8</td><td>5</td></tr><tr><td>TransR</td><td>0.053</td><td>0.152</td><td>0.019</td><td>0.000</td><td>3</td><td>3</td><td>4</td><td>5</td></tr><tr><td>TransH</td><td>0.026</td><td>0.074</td><td>0.015</td><td>0.000</td><td>5</td><td>5</td><td>6</td><td>5</td></tr><tr><td>TransD</td><td>0.022</td><td>0.064</td><td>0.018</td><td>0.000</td><td>7</td><td>6</td><td>5</td><td>5</td></tr><tr><td>ComplEx</td><td>0.042</td><td>0.084</td><td>0.028</td><td>0.010</td><td>4</td><td>4</td><td>3</td><td>3</td></tr><tr><td>DistMult</td><td>0.025</td><td>0.063</td><td>0.010</td><td>0.004</td><td>6</td><td>7</td><td>7</td><td>4</td></tr><tr><td>RotatE</td><td>0.150</td><td>0.357</td><td>0.150</td><td>0.054</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>BioBRIDGE</td><td>0.189</td><td>0.380</td><td>0.176</td><td>0.071</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table></body></html>  

Table 15: Cross-modality prediction performance for triple type: “Protein - Target - Drug”.   


<html><body><table><tr><td>Method</td><td>MRR</td><td>Hit@10</td><td>Hit@3</td><td>Hit@1</td><td>MRRRank</td><td>Hit@10Rank</td><td>Hit@3Rank</td><td>Hit@1Rank</td></tr><tr><td>TransE</td><td>0.033</td><td>0.086</td><td>0.016</td><td>0.000</td><td>8</td><td>8</td><td>8</td><td>5</td></tr><tr><td>TransR</td><td>0.069</td><td>0.204</td><td>0.058</td><td>0.000</td><td>4</td><td>3</td><td>4</td><td>5</td></tr><tr><td>TransH</td><td>0.043</td><td>0.127</td><td>0.018</td><td>0.000</td><td>7</td><td>6</td><td>6</td><td>5</td></tr><tr><td>TransD</td><td>0.049</td><td>0.150</td><td>0.018</td><td>0.000</td><td>5</td><td>5</td><td>6</td><td>5</td></tr><tr><td>ComplEx</td><td>0.079</td><td>0.184</td><td>0.075</td><td>0.021</td><td>3</td><td>4</td><td>3</td><td>3</td></tr><tr><td>DistMult</td><td>0.044</td><td>0.104</td><td>0.035</td><td>0.008</td><td>6</td><td>7</td><td>5</td><td>4</td></tr><tr><td>RotatE</td><td>0.125</td><td>0.258</td><td>0.131</td><td>0.051</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>BioBRIDGE</td><td>0.172</td><td>0.299</td><td>0.181</td><td>0.107</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table></body></html>  

Table 16: Cross-modality prediction performance for triple type: “Disease - Associated with - Protein”.   


<html><body><table><tr><td>Method</td><td>MRR</td><td>Hit@10</td><td>Hit@3</td><td>Hit@1</td><td>MRRRank</td><td>Hit@10Rank</td><td>Hit@3Rank</td><td>Hit@1Rank</td></tr><tr><td>TransE</td><td>0.010</td><td>0.015</td><td>0.002</td><td>0.000</td><td>8</td><td>8</td><td>7</td><td>5</td></tr><tr><td>TransR</td><td>0.029</td><td>0.066</td><td>0.007</td><td>0.000</td><td>5</td><td>5</td><td>5</td><td>5</td></tr><tr><td>TransH</td><td>0.014</td><td>0.020</td><td>0.003</td><td>0.000</td><td>6</td><td>6</td><td>6</td><td>5</td></tr><tr><td>TransD</td><td>0.013</td><td>0.018</td><td>0.002</td><td>0.000</td><td>7</td><td>7</td><td>7</td><td>5</td></tr><tr><td>ComplEx</td><td>0.048</td><td>0.106</td><td>0.031</td><td>0.010</td><td>3</td><td>4</td><td>4</td><td>3</td></tr><tr><td>DistMult</td><td>0.047</td><td>0.113</td><td>0.035</td><td>0.010</td><td>4</td><td>3</td><td>3</td><td>3</td></tr><tr><td>RotatE</td><td>0.076</td><td>0.172</td><td>0.059</td><td>0.021</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>BioBRIDGE</td><td>0.081</td><td>0.184</td><td>0.071</td><td>0.025</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table></body></html>  

Table 17: Cross-modality prediction performance for triple type: “Drug - Contraindication - Disease”.   


<html><body><table><tr><td>Method</td><td>MRR</td><td>Hit@10</td><td>Hit@3</td><td>Hit@1</td><td>MRRRank</td><td>Hit@10Rank</td><td>Hit@3Rank</td><td>Hit@1Rank</td></tr><tr><td>TransE</td><td>0.010</td><td>0.015</td><td>0.002</td><td>0.000</td><td>8</td><td>8</td><td>7</td><td>5</td></tr><tr><td>TransR</td><td>0.029</td><td>0.066</td><td>0.007</td><td>0.000</td><td>5</td><td>5</td><td>5</td><td>5</td></tr><tr><td>TransH</td><td>0.014</td><td>0.020</td><td>0.003</td><td>0.000</td><td>6</td><td>6</td><td>6</td><td>5</td></tr><tr><td>TransD</td><td>0.013</td><td>0.018</td><td>0.002</td><td>0.000</td><td>7</td><td>7</td><td>7</td><td>5</td></tr><tr><td>ComplEx</td><td>0.048</td><td>0.106</td><td>0.031</td><td>0.010</td><td>3</td><td>4</td><td>4</td><td>3</td></tr><tr><td>DistMult</td><td>0.047</td><td>0.113</td><td>0.035</td><td>0.010</td><td>4</td><td>3</td><td>3</td><td>3</td></tr><tr><td>RotatE</td><td>0.076</td><td>0.172</td><td>0.059</td><td>0.021</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>BioBRIDGE</td><td>0.081</td><td>0.184</td><td>0.071</td><td>0.025</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table></body></html>  

Table 18: The statistics of the built cross-specifes retrieval dataset.   


<html><body><table><tr><td></td><td>Mouse Protein - Mouse Phenotype</td><td></td><td>MouseProtein-Human Phenotype Item Test</td></tr><tr><td>Item</td><td>Train</td><td>Test 6,068</td><td>MGI 1,615</td></tr><tr><td>MGI MPI</td><td>6,069 28</td><td>28</td><td>HPO 353</td></tr><tr><td>Pairs</td><td></td><td>37,937</td><td>Pairs 3,515</td></tr><tr><td>MeanMPI</td><td>38,705 6.38</td><td>6.26</td><td>MeanHPO 2.18</td></tr></table></body></html>  

# E MOUSE PROTEIN AND PHENOTYPE DATASET  

We retrieved the paired mouse protein and phenotype data from https://www.informatics. jax.org/downloads/reports/HMD_HumanPhenotype.rpt, yielding a total of 29,686 paired mouse protein ID (MGI), human protein ID, and mouse phenotype ID (MPI), namely data1. We then retrieved the detailed descriptions of mouse phenotypes from https:// www.informatics.jax.org/downloads/reports/MPheno_OBO.ontology, yielding 14,201 MPI and their defnition, namely data2.  

To obtain the exact sequence structure of mouse proteins, we called UniProt https://www. uniprot.org/id-mapping mapping system, choosing “MGI” under “Organism-specifc databases” as the source database, “UniproKB/Swiss-Prot” as the target database. This process maps MGI to the corresponding protein sequences, namely data3.  

We only keep MGI that is with protein sequence referring to data3 from data1. Also, we only keep MGI in data1 that is linked to more than one MPI. Finally, we conduct a flter from data1 to keep human proteins that are in the PrimeKG data and link to more than one human protein. The obtained dataset then has 14,024 MGI, 14,200 MPI, 1,582 Human Proteins, and 3,279 Human Phenotypes in total.  

# F CASE STUDY: MULTIMODAL Q&A  

Table 19: Additional case studies of multi-modal Q&A based on BioBRIDGE: Input a molecule and a question, retrieve the relevant diseases and proteins, and prompt LLM to answer. The references are drawn for the input molecule-based drug pages from DrugBank.   


<html><body><table><tr><td>Input Molecule C[C@H](CI=NOC</td><td>Question What are the asso-</td><td>Retrieved Disease osteoporosis</td><td>Retrieved Protein MAPK14</td><td>Answer The associated diseases and the proteins this molecule targets suggest that</td><td>Reference</td></tr><tr><td>C1=CC=C(C(F)=C1) C1=CC=CC=C1</td><td>(NC(=N)N2CCOCC2)=C1ciated conditions and pharmaco- dynamics of this onp</td><td>postmenopausal osteoporosis psoriasis allergic contact dermatitis atopic eczema rheumatoid arthritis osteonecrosis allergic asthma systemic lupus erythematosus (disease) congestive heart failure</td><td>CDK2 MAPK1 MAPK8 MAPK3 EP300 MAPK9 CREBBP GSK3B MAPK10</td><td>Rimacalib has been used in this small molecule drug may have immunomodulatory, anti-inflammatory, and anti-arthritic effects. The drug may inhibit the activity of MAPK14, which plays a role in the pro- duction of pro-infammatory cytokines. This inhibition may help reduce in- flammation in conditions such as psoriasis, allergic contact dermatitis, atopic eczema, rheumatoid arthritis, and allergic asthma. The drug may also inhibit th activity ofCDK2,whichis involved n thee ulation of the cell cycle and proliferation of immune cells. This inhibition may help suppress the immune system and reduce inflammation in systemic lupus erythematosus (disease). Aditionally, the drug may inhibit the activity of otherMAPKfamily mem- bers such as [..], which are involved in the regulation of immune responses and inflammation. The drug may also activate EP300 and CREBBP, which are transcriptional co-activators that can promote the expression of genes involved in anti- inflammatory responses. Overall, the main pharmacodynamics of this small molecule drug may be</td><td>trials studying the treatment of Rheumatoid Arthritis (RA). [DrugBank: DB12571]</td></tr><tr><td>N[C@H](C(=O)N1 [C@H]2C[C@H]2C ciated conditions [C@H]1C#N)C12CC3CC and (CC(O)(C3)C1)C2</td><td>What are the asso- pharmaco- dynamics of this onp</td><td>type 2 diabetes mellitus coronary artery disease bipolar disorder bipolar II disorder bipolar depression bipolar I disorder obesity disorder dysthymic disorder monogenic obesity neurotic disorder</td><td>DPP4 F2 FAP F9 PRSS2 PLAT F10 ACE DPP6 PRSS1</td><td>a potential treatment option for various conditions such as psoriasis, allergic contact dermatitis, atopic eczema Saxagliptin isa dipeptidyl main possible associated conditions of this small molecule drug are type 2 diabetebeityronaryaerydiseaseroti disorderdthmi order, monogenic obesity, diabetic ketoacidosis, and bipolar disorders. The main pharmacodynamics of this small molecule drug are its inhibitory effects on dipeptidy1 peptidase 4 (DPP4) and fibroblast activation protein al- pha (FAP), which may help manage type 2 diabetes and obesity. The drug may also have some effects on the coagulation cascade, potentially reducing the risk of coronary artery disease and other coagulation-related disorders. Additionall the drugmay have some anti-inflammatory effects throughit inhibition of FAP, which may contribute to its efficacy in managing some </td><td>antidiabetic for the treatment of type 2 diabetes. [DrugBank: DB06335]</td></tr><tr><td>CN/ C(NCCSCC1=CC=C (CN(C)C)01)= C/[N+]([O-])=O</td><td>Which diseases can this molecule treat and why?</td><td>diabetic ketoacidosis conjunctivitis (disease) duodenal ulcer (disease) pulmonary embolism (disease) gastroenteritis chronic duodenal ileus peptic ulcer disease intralobar congenital monary sequestration Echovirus infectious disease trachoma lower respiratory tract disease</td><td>NOS2 NOS3 GSTP1 GSTA2 GSTA1 GSTM2 pul- GSTM1 GSTA5 GSTM4 PTGS2</td><td>neurotic and mood disorders. Based onthe target protensandassociateddiseasesisted hemain oble Ranitidine is a histamine H2 associated conditions of this small molecule drug are: antagonist used to treat duo- 1. Conjunctivitis (disease) 2. Pulmonary embolism (disease) 3. Gastroenteritis 4. Chronic duodenal ileus 5. Peptic ulcer disease 6. Echovirus infectious disease The main pharmacodynamics of this small molecule drug would depend on its interaction with the target proteins. As it targets nitric oxide synthase 2 and 3, it may have vasodilation and anti-inflammatory effects. As it targets glutathione S-transferase pi 1, alpha 2, alpha 1, mu 2, mu 1, alpha 5, mu anti-mmatoryffectsdinallyasmalllulua</td><td>denal ulcers, Zollinger-Elison syndrome, gastric ulcers, GERD, and erosive esophagi- tis. [DrugBank: DB00863]</td></tr><tr><td>[H][C@@]120C3=C What are the asso- (OC)C=CC4=C3[C@@] 11CCN(C)[C@]([H) (C4)[C@]1([H]) C=C[C@@H]20</td><td>ciated conditions and pharmaco- dynamics of this drug?</td><td>vasomotor rhinitis allergic rhinitis seasonal allergic rhinitis perennial allergic rhinitis common cold atopic conjunctivitis nasopharyngitis papillary conjunctivitis giant papillary conjunctivitis vernal conjunctivitis pharyngitis angioedema</td><td>OPRM1 OPRK1 OPRD1 OPRL1 HTR2C HTR2A DRD2 CHRM2 HTR2B DRD3</td><td>have a good ability to cross cell membranes and reach its targets quickly. Based on the associated diseases and the proteins this molecule targets,the main possible associated conditions of this small molecule drug are pain, asthma, and allergic reactions. The main pharmacodynamics of this small molecule drug are its activity as an opioid receptor agonist, which can reg- ulate the release of neurotransmitters involved in pain, mood, respiration, and immune function. It can also have anti-inflammatory and anti-allergic effects, although the exact mechanisms are not fully understood. Therefore, this drug may be effective in treating pain and asthma, as well as allergic conditions such as vasomotor rhinitis, allergic rhinitis, and allergic asthma.</td><td>Codeine is utilized as a cen- tral analgesic, sedative, hyp- notic, antinociceptive, and an- tiperistaltic agent, and is also recommended in certain dis- eases with incessant coughing. [Drugbank: DB00318]</td></tr><tr><td>CN[C@@]I(C) [C@@H]2CC[C@@H] (C2)C1(C)C and</td><td>What are the asso- ciated conditions dynamics of this onp</td><td>allergic asthma alcohol dependence substance abuse pharmaco- substance dependence alcohol abuse heroin dependence cannabis dependence cocaine abuse cocaine dependence</td><td>SLC6A3 HTR1A CHRM2 SLC6A2 CHRM3 SLC6A4 DRD2 CHRM4 CHRM5 HTR1B tions.</td><td>The small molecule drug described targets multiple proteins involved in the regulation of various neurotransmitters and their receptor pathways. The as- sociated diseases and conditions that this drug could possibly address include drug and alcohol dependence, substance abuse and dependence, and related disorders such as anxiety and depression. The main pharmacodynamics of this drug involve the modulation of the neu- rotransmitter systems of serotonin, dopamine, and acetylcholine, which are involved in the regulation of mood, emotional response, reward-seeking be- haviors, and cognitive function. The specific actions of this drug on each of the target proteins are not explicitly stated in the given information, but based on the known functions of these proteins, this drug could potentially regulate therelase,eptak,an sigalingf therotranmitteradin t alleviation of symptoms associated with the associated diseases and condi-</td><td>Dexmecamylamine has been used in trials studying the basic science and treatment f PatientsDepression,Drug Abuse, Pharmacokinetics, and Renal Impairment, among oth- ers. [DrugBank: DB11807]</td></tr><tr><td>(o=）so（o)[1v]0 (=O)OC[C@H]10 [C@@H](O[C@]2 (COS(=O)(=O)O[AI] (0)0)0 [C@H](OS(=O)(=O) [H@lo(O[Iv]o (OS(=O) (o(o)[1v]o(o=) [C@ @H]2OS(=O)(=O)O[A1] (O)O)[C@H](OS(=O)(=O) O[AI](O)O)[C@@H](OS(=O) (=O)O[A1](O)O)[C@@H] 10S(=0)(=0)O[AI](O)0</td><td>What are the asso- ciated conditions and pharmaco- drug?</td><td>esophagitis (disease) gallbladder disease gallbladder disease 2 dynamics of this gallbladder disease 3 cirrhosis of liver peptic ulcer disease kidney disease diarrheal disease gastroesophageal reffux disease asthma pyoureter inherited hemoglobinopathy acute myocardial infarction hemoglobinopathy acquired hemoglobinopathy</td><td>TF SOD2 TGFB1 IL6 LTF MMP2 TFRC ALB VEGFA CAT</td><td>Based on the information provided, it appears that the small molecule dug may have anti-inflammatory and antioxidant effects as it targets several pro- teins involved in inflammation and oxidative stress, including interleukin 6, superoxide dismutase 2,transforming growth factor beta 1, and vascular n- dothelial growth factor A. In terms of associated conditions, the drug may be effective in treating dis- eases that involve inflammation and oxidative stress, such as cirrhosis of the liver, ppic ulcer disease, diarrheal disease, a kidey diseaseItmay al have potential in treating gastroesophageal reflux disease and gallbladder diseases, which are also associated with inflammation and oxidative stress. Howeverits mportant tonote that these are just speculative associatin based on the limited information provided. portant in the healing of the</td><td>ing of duodenal ulcers, reliv- ing painful infammation by creating a protetive mechan- ical barrier between the lin- ing or skin of the gastrointesti- nal tract and damaging sub- stances 2. In addition, su- cralfate acts to increase lev- els of growth factors locally, and also causes an increase in prostaglandins which are im- mucosa (lining) of the gas- trointestinal tract 2. [Drug- Bank: DB00364]</td></tr></table></body></html>  

Table 20: Case study of multi-modal generation based on BioBRIDGE: Input the target condition and the intended mechanism, retrieve the relevant proteins, and prompt LLM to generate the target small molecule drug.   


<html><body><table><tr><td>Target Condition</td><td>Target Effect</td><td>RetrievedProtein</td><td>GeneratedMolecule</td><td>MostSimilar Drug</td></tr><tr><td>Depressive disorder</td><td>being a selective dopamine receptor antagonist, hence inhibiting dopamin- ergic hyperactivity.</td><td>DRD2 HTR1A OPRM1 HTR2A HTR2C IL1B BDNF CNR1</td><td>CN1CCIC@H]2CN(C[C@@H]3Cc 400c(F)04[C@@H]3C@H]2C1</td><td>HO [DB00854] Levorphanol [C@@H]2CC2=C1C=C(O)C=C2</td></tr><tr><td>Depressive disorder</td><td>The drug treats depressive disorder by being a selective dopamine receptor antagonist,hence inhibiting dopamin- ergic hyperactivity.</td><td>PENK TPH1 DRD2 HTR1A OPRM1 HTR2A HTR2C IL1B BDNF CNR1 PENK TPH1</td><td>CN1CCC[C@H]1g2=CC(=C(C=C 2)OC)OC3=CC=CC=C3</td><td>Levorphanol is an opioid analgesic used to treat moderate to severe pain. CC1=CC(=CC(C)=C1) CC-C2FCIN-OC=C1 C@@H1CCCN1CC1=CC=C(OC2=CC- Ly2456302 has been used in trials studying the health services research and basic science of Anxiety Disor-</td></tr><tr><td>Breast cancer</td><td>We want to design a drug that can target one of the following proteins: [proteins..] to treat breast cancer.</td><td>BRCA2 NOTCH2 NOTCH1 TYMS EGFR NOTCH3 TP53 CDKN2A ESR2 NCOA1</td><td>CCCCCCCCCCCNC1=NC(=O)N([C@ @H]2O[C@H](C)[C@@H](O) [C@H]2O)C=C1F</td><td>ders and Alcohol Dependence. CCCCCOC(=O)NC1=NC(=O)N(C=C1F) H,C [DB01101] Capecitabine [C@@H]10(C8H(C)C@@H(O)(C@H]10 Capecitabine iS a nucleoside inhibitor indicated to in-</td></tr><tr><td>Breast Cancer</td><td>target one of the following proteins: [proteins..] to treat breast cancer.</td><td>We want to design a drug that can BRCA2 NOTCH2 NOTCH1 TYMS EGFR NOTCH3 TP53 CDKN2A ESR2 NCOA1 ERBB2</td><td>HO O=C1C2=C(O)C=CC([(N+)=O)[O-])=C2C(=O)C2=C([N +](=O)O-])C=CC([N+]{(=O)[O-])=C12</td><td>metabolic treat different gastrointestinal, cluding pancreatic cancer, and breast cancer. OCCNCCNC1=CC=C(NCCNCCO)C2=C1C(=O)C1=C( [DB01204] Mitoxantrone C(O)=CC=C10)C2=0 Mitoxantrone is a chemotherapeu- tic agent used for the treatment of secondary progressive,progressive relapsing, or worsening relapsing- remittingmultiple sclerosis.</td></tr></table></body></html>  

![](images/6d7c2c87393ad4143074abbea7af8b846f5d8b26c0608a6f6ed6444f491b83ae.jpg)  

# G CASE STUDY: PROMPT  

Listing 1: The prompt for prompting language models to answer user questions takes the drug molecule structure as the input.  

prompt $=$ """Drug molecule structure: [START_I_SMILES] {smiles} [END_I_SMILES]  

Target proteins: {protein_names}  

Associated diseases: {disease_names}  

Consider the associated diseases and the proteins this molecule targets, {input_question}  

Listing 2: The prompt for prompting language models to generate drug molecule guided by user inputs in natural language.  

prompt $=$ """The drug may be targeting the proteins:  

{protein_names}  

{text_guidance}  

Generate the most possible SMILES structure of this drug. """  

# H ADDITIONAL RELATED WORKS  

Knowledge Graph Learning. Knowledge graph embedding learning algorithms were proposed for KG completion tasks (Han et al., 2018), i.e., given the head node and relation, predict the most probable tail node. These approaches assume a fxed set of nodes and do not handle inductive prediction, i.e., prediction for nodes not seen in the training. More recent efforts utilize graph neural networks to handle inductive setup (Teru et al., 2020; Liu et al., 2021), which generate embeddings for new nodes by aggregating its neighbors’ embeddings. Nevertheless, they still do not handle isolated new nodes with no link to known nodes. Also, the inherent node properties, e.g., the sequence structure of protein nodes, were usually neglected in the encoding. On the contrary, the proposed BioBRIDGE learns from a KG, representing new nodes with base FMs and transforming the embeddings via bridge modules without needing neighbor graph inputs.  

Multimodal Learning. Early works in multi-modal learning focus on cross-modal retrieval, such as based on Canonical Correlation Analysis (CCA) to learn a common representation space for different modalities (Rasiwasia et al., 2010). In the era of deep learning, researchers turned to fuel deep models with large cross-modal data, such as text and image, to get better performances (Radford et al., 2021). Multimodal learning was also introduced to knowledge graph embeddings, such as in (Xie et al., 2019; Mousselly-Sergieh et al., 2018; Lu et al., 2022).z However, these methods primarily focus on visual and textual modalities, which may not directly suit our specifc application needs. A critical limitation of these methods is their confnement to predicting nodes within the training KG during the testing phase because these methods all assume the KG is fxed. In contrast, BioBridge stands out by leveraging base unimodal Feature Models (FMs) to process inputs from any modality present in the KG. It can then convert these inputs into any desired target modality.  

# I TRAINING SETUPS  

In this paper, we aim to prove that bridging uni-modal FMs is feasible. As such, we deliberately choose simple architectures, like transformers, to transform input embeddings and original InfoNCE loss to learn the transformation modules. All are kept to a minimum but necessary to make BioBridge work.  

We did experiments to check the sensitivity of BioBridge w.r.t. hyperparameters; we chose the batch size to be in $\{512,1024,4096\}$ , and training epochs to be $\{10,50,100\}$ . We found the performance was not signifcantly different for the tried batch size. The method turned out to be converging within 50 epochs, and training with more epochs does not lead to further improvement. As such, we keep the same set of hyperparameters for BioBridge across all experiments: batch size 4096, training epochs 50, and learning rate 1e-4.  

In our experiments, we also did the ablations for the bridge module: Variant 1: in Eq. (1), removing the residual connection, i.e., using $h=\psi(z,c_{i},c_{j},r)$ . Variant 2: in Eq. (1), using RotatE transformation, i.e., using $h=z\circ\psi(z,c_{i},c_{j},r)$ . In our experiments, Variant 1 failed to converge; Variant 2 obtained a worse performance than the additive transformation in Eq. (1).  